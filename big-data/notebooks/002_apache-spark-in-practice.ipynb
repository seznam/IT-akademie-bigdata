{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark in practice - IT Academy 2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seznam/IT-akademie-bigdata/blob/main/big-data/notebooks/002_apache-spark-in-practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What you'll put your hands on in this notebook\n",
        "\n",
        "- reading data from the Parquet format\n",
        "- linking two separate data sets together based on a common field\n",
        "- writing simple aggregation"
      ],
      "metadata": {
        "id": "eVmi7UboZsSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data & Spark session"
      ],
      "metadata": {
        "id": "Sarump79aV6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following snippet will download our example dataset which you'll be working with:"
      ],
      "metadata": {
        "id": "m6PfAenLaZnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmtPsYHX7xX"
      },
      "outputs": [],
      "source": [
        "!test -f example-dataset.tar.xz || wget https://github.com/seznam/IT-akademie-bigdata/raw/main/big-data/data/example-dataset.tar.xz\n",
        "!test -d example-dataset || tar -xf example-dataset.tar.xz\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's install Spark on PySpark:"
      ],
      "metadata": {
        "id": "G8XblEIbafX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Spark\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "!test -f spark-3.2.1-bin-hadoop2.7.tgz || wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!test -d spark-3.2.1-bin-hadoop2.7 || tar -xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "\n",
        "# Setup pyspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "\n",
        "# Create new SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "zzn3RQlrauVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just some handy functions to keep the code cells clean later on\n",
        "# Note that we don't use asterisk (*) because then the Colab completion doesn't work\n",
        "from pyspark.sql.functions import col, floor, udf, explode"
      ],
      "metadata": {
        "id": "bL8Ua8jyaztf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data\n",
        "\n",
        "Now it's time for you to come up with what you've learned from the previous [Introduction to Apache Spark - IT Academy 2022](https://colab.research.google.com/github/seznam/IT-akademie-bigdata/blob/main/big-data/notebooks/001_introduction_to_apache_spark.ipynb) notebook.\n",
        "\n",
        "Let us help you for starters by looking into the structure of the data we have prepared for you."
      ],
      "metadata": {
        "id": "b1CBaSA6a-Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l example-dataset/"
      ],
      "metadata": {
        "id": "bIk9BOmDdRMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, we have two directories `clicks` and `pageviews`. There were taken from Seznam's ad division, so a click means some user clicked an ad, while an impression means an ad was loaded by the browser and rendered.\n",
        "\n",
        "Let's look into the directories to see the data format:"
      ],
      "metadata": {
        "id": "GyQZylO9dZ6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l example-dataset/*"
      ],
      "metadata": {
        "id": "Xbl3haL6fI5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the data are using Parquet format. And here comes your first task:\n",
        "- load clicks parquet directory into one DataFrame (named `clicks`)\n",
        "- and pageviews directory into other DataFrame (named `pageviews`)\n",
        "\n",
        "*HINT: [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet) might be handy if autocompletion does not suffice*"
      ],
      "metadata": {
        "id": "jwNmhIGRfXz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks = "
      ],
      "metadata": {
        "id": "gvDBhvbSftw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageviews = "
      ],
      "metadata": {
        "id": "M_gUvrgCgLTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the schema and the data to verify you have loaded it successfully"
      ],
      "metadata": {
        "id": "0dU8s1cggPVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks.printSchema()\n",
        "clicks.show()"
      ],
      "metadata": {
        "id": "V06r_CJzgVoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageviews.printSchema()\n",
        "pageviews.show()"
      ],
      "metadata": {
        "id": "vZ4YQhIigZKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! At this point, you're ready to go on to the second part."
      ],
      "metadata": {
        "id": "hYf0m_wXgj6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linking two datasets together"
      ],
      "metadata": {
        "id": "KayA17Xngpsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the schemas of both DataFrames, there are some fields, which we can use to link the data together.\n",
        "\n",
        "One of them is `click.ImpressionTimestamp` column, which should correlate to `pageview.timestamp` column. We will use this to link the two datasets together.\n",
        "\n",
        "The second one is a bit tricky to understand, because first you need to understand, what is an impression within our dataset.\n",
        "- single line of `pageviews` DataFrame represents a single rendering result from the browser, but it potentially includes more ads at once, which is why there is an column named `randomIds`, which is actually an array of integers.\n",
        "- each `randomId` can be considered as a single ad\n",
        "- since one line of `clicks` DataFrame represents a single click on a single ad, we can link `pageviews` to `clicks` only after we *explode* our `randomIds` array\n",
        "  - what do we mean by *exploding* the array?\n",
        "  - well, it's like flattening the structure, so that we get \"more lines\" in the DataFrame at the end, so that we have one pageview with exactly one randomId\n",
        "\n",
        "That said, we now know we can also link `click.RandomId` field with exploded `pageview.randomIds[]` array."
      ],
      "metadata": {
        "id": "ZXEjR06LgvXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See function docs you should use:\n",
        "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html\n",
        "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html"
      ],
      "metadata": {
        "id": "YEeTa8a8kjYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exploded_pageviews = "
      ],
      "metadata": {
        "id": "M-QDlRT2jk5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we proceed to joining two data frames, we need to unify correlated column names\n",
        "exploded_pageviews = exploded_pageviews \\\n",
        "                        .drop('randomIds') \\\n",
        "                        .withColumnRenamed('timestamp', 'impressionTimestamp')"
      ],
      "metadata": {
        "id": "zx4ODX4pk7zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what we've got:"
      ],
      "metadata": {
        "id": "LyJMqvf-l70E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exploded_pageviews.printSchema()\n",
        "exploded_pageviews.show()"
      ],
      "metadata": {
        "id": "mvsNXh0al20D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've exploded pageviews using `randomIds`, it's time for you to join these DataFrames by `randomId` and  `impressionTimestamp` fields. Please consult the docs:\n",
        "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html"
      ],
      "metadata": {
        "id": "s8vLGkpeldud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked_pageviews = "
      ],
      "metadata": {
        "id": "jUEtcn_KkRlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid ambiguity, rename Timestamp column from clicks\n",
        "linked_pageviews = linked_pageviews \\\n",
        "                      .withColumnRenamed('Timestamp', 'ClickTimestamp') "
      ],
      "metadata": {
        "id": "TFr90UdgkysW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we got it right:"
      ],
      "metadata": {
        "id": "RBvCwC0ol0ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked_pageviews.printSchema()\n",
        "linked_pageviews.show()"
      ],
      "metadata": {
        "id": "WaH4w5J9mLY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task\n",
        "\n",
        "- create also unlinked DataFrame, which will contain all these pageviews or clicks, which were not linked by the other DataFrame"
      ],
      "metadata": {
        "id": "gZ1SY_W8nH07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: using just \"DataFrame.join\" method is enough\n",
        "\n",
        "unlinked_pageviews =\n",
        "\n",
        "unlinked_pageviews.printSchema()\n",
        "unlinked_pageviews.count()"
      ],
      "metadata": {
        "id": "KFrZFKvznO-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing basic aggregation"
      ],
      "metadata": {
        "id": "Gr3ZaFdunEsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task here is to create a summary statistics of how many clicks were registered per each `AdId`.\n",
        "\n",
        "For example:\n",
        "\n",
        "| AdId | Clicks |\n",
        "| - | - |\n",
        "| 1002 | 30 |\n",
        "| 586 | 2 |\n",
        "| ... | ... |"
      ],
      "metadata": {
        "id": "hp46e2kmokoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hint: Use group by aggregation from the [Introduction to Apache Spark - IT Academy 2022](https://colab.research.google.com/github/seznam/IT-akademie-bigdata/blob/main/big-data/notebooks/001_introduction_to_apache_spark.ipynb) notebook.*"
      ],
      "metadata": {
        "id": "JjnzfNNopH0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks_by_ad_id =\n",
        "\n",
        "clicks_by_ad_id.printSchema()\n",
        "clicks_by_ad_id.show()"
      ],
      "metadata": {
        "id": "Upq9NvNAqml2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, you've got it!\n",
        "\n",
        "Now, let's create an UDF to extract UNIX timestamp of the nearest hour (rounded to floor) from the `ClickTimestamp` field (within the `linked_pageviews` DataFrame) and print the number of linked pageviews per each hour."
      ],
      "metadata": {
        "id": "ll5YRa7dqDes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hour(click_timestamp):\n",
        "  # TODO\n",
        "  return None"
      ],
      "metadata": {
        "id": "uofRXD9owIdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_hour_udf = udf(extract_hour)"
      ],
      "metadata": {
        "id": "aC47929Xwk7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Apply this UDF to linked_pageviews DataFrame and compute count of them per each hour (and print first few lines)\n",
        "\n",
        "linked_pageviews"
      ],
      "metadata": {
        "id": "uYEKWQenwpnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
